{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import git\n",
    "import ast\n",
    "import re\n",
    "import subprocess\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of a repository line : \n",
    "___________\n",
    "'Name', 'Description', 'URL', 'Stars', 'Forks', 'Lines of Code'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ AND WRITE CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(repositories, filename):\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        fieldnames = [\"name\", \"html_url\", \"lines_of_code\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for repo in repositories:\n",
    "            writer.writerow({\n",
    "                \"name\": repo[\"name\"],\n",
    "                \"html_url\": repo[\"html_url\"],\n",
    "                \"lines_of_code\": get_lines_of_code(repo)\n",
    "            })\n",
    "\n",
    "def load_from_csv(filename):\n",
    "    repositories = []\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            repositories.append({\n",
    "                \"name\": row[\"name\"],\n",
    "                \"html_url\": row[\"html_url\"],\n",
    "                \"lines_of_code\": int(row[\"lines_of_code\"])\n",
    "            })\n",
    "    return repositories\n",
    "\n",
    "def get_lines_of_code(repo):\n",
    "    return repo[\"size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peruse github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_repositories(query, min_lines_of_code, token=None):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "\n",
    "    # Set up the GitHub search API URL\n",
    "    search_url = \"https://api.github.com/search/repositories\"\n",
    "    params = {\"q\": query, \"sort\": \"stars\", \"order\": \"desc\"}\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(search_url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        repositories = response.json()[\"items\"]\n",
    "        filtered_repos = [repo for repo in repositories if get_lines_of_code(repo) > min_lines_of_code]\n",
    "        return filtered_repos\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloning And word search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_classes_with_keyword(file_path, keyword):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    tree = ast.parse(content)\n",
    "    \n",
    "    class_count = 0\n",
    "    \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            if keyword.lower() in node.name.lower():\n",
    "                class_count += 1\n",
    "    \n",
    "    return class_count\n",
    "\n",
    "def clone_and_analyze_repositories(repositories, keyword):\n",
    "    result = []\n",
    "\n",
    "    for repo in repositories:\n",
    "        repo_name = repo['Name']\n",
    "        repo_url = repo['URL']\n",
    "        repo_path = f\"repos/{repo_name}\"\n",
    "\n",
    "        try:\n",
    "            # Clone the repository\n",
    "            git.Repo.clone_from(repo_url, repo_path)\n",
    "\n",
    "            # Analyze the code\n",
    "            total_classes_with_keyword = 0\n",
    "\n",
    "            for root, dirs, files in os.walk(repo_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('.py'):\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        total_classes_with_keyword += count_classes_with_keyword(file_path, keyword)\n",
    "\n",
    "            result.append({'Name': repo_name, 'ClassesWithKeyword': total_classes_with_keyword})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing repository '{repo_name}': {e}\")\n",
    "        finally:\n",
    "            None\n",
    "            # Clean up: Remove the cloned repository\n",
    "            # if os.path.exists(repo_path):\n",
    "                # shutil.rmtree(repo_path)\n",
    "\n",
    "    return result\n",
    "\n",
    "def count_keyword_occurrences(repo, keyword):\n",
    "    occurrences = 0\n",
    "    # Get the repository's contents\n",
    "    contents_url = repo[\"contents_url\"].replace(\"{+path}\", \"\")\n",
    "    response = requests.get(contents_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        files = response.json()\n",
    "        for file in files:\n",
    "            if file[\"type\"] == \"file\" and file[\"name\"].endswith(\".py\"):\n",
    "                # Download the Python file\n",
    "                file_url = file[\"download_url\"]\n",
    "                file_content = requests.get(file_url).text\n",
    "\n",
    "                # Count occurrences of the keyword using regular expressions\n",
    "                occurrences += len(re.findall(fr'\\b{keyword}\\b', file_content))\n",
    "    \n",
    "    return occurrences\n",
    "\n",
    "def count_word_all_occurrences(repo, word, token=None):\n",
    "    occurrences = 0\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "\n",
    "    # Get the repository's contents\n",
    "    contents_url = repo[\"contents_url\"].replace(\"{+path}\", \"\")\n",
    "    response = requests.get(contents_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        files = response.json()\n",
    "        for file in files:\n",
    "            if file[\"type\"] == \"file\":\n",
    "                # Download the file\n",
    "                file_url = file[\"download_url\"]\n",
    "                file_content = requests.get(file_url, headers=headers).text\n",
    "\n",
    "                # Count occurrences of the word using regular expressions\n",
    "                occurrences += len(re.findall(fr'\\b{word}\\b', file_content, flags=re.IGNORECASE))\n",
    "    \n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repo(repo):\n",
    "        repo_url = repo['html_url'] + '.git'\n",
    "        local_repo_path = os.path.join(os.getcwd(), repo['name'])\n",
    "        subprocess.run(['git', 'clone', repo_url, local_repo_path])\n",
    "        return local_repo_path\n",
    "\n",
    "def count_word_occurrences(local_repo_path, word):\n",
    "    occurrences = 0\n",
    "\n",
    "    # Iterate through all files in the local repository\n",
    "    for root, dirs, files in os.walk(local_repo_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Read the file content and count occurrences of the word\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                file_content = f.read()\n",
    "                occurrences += len(re.findall(fr'\\b{word}\\b', file_content, flags=re.IGNORECASE))\n",
    "    \n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.github.com/search/repositories'\n",
    "techniques = []\n",
    "metrics = []\n",
    "csv_filename = 'repositories_py.csv'\n",
    "\n",
    "query = 'devops' #\"devops language:python\"\n",
    "min_lines_of_code = 10000\n",
    "token = None  # Replace with your GitHub token (optional)\n",
    "\n",
    "repositories = search_repositories(query, min_lines_of_code, token)\n",
    "save_to_csv(repositories, csv_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = map(lambda x : x[\"name\"], repositories)\n",
    "techniques = ['testing', 'interview', 'questionnaire', 'survey', 'observation','theatre', 'prototype', 'incident report', 'developper as customers', 'customer pairing', 'walkthrough', 'ads', 'beta', 'operational', 'a/b', 'social network', 'crowd-funding']\n",
    "metrics = ['acquisition', 'activation', 'retention', 'referal', 'revenue']\n",
    "\n",
    "metric_res = {'metrics': metrics}\n",
    "techniques_res = {'techniques': techniques}\n",
    "\n",
    "for repo in repositories:\n",
    "    name = repo[\"name\"]\n",
    "    metric_res[name] = []\n",
    "    techniques_res[name] = []\n",
    "    local_repo = clone_repo(repo)\n",
    "\n",
    "    for metric in metrics:\n",
    "        metric_res[name].append(count_word_occurrences(local, metric))\n",
    "\n",
    "    for t in techniques:\n",
    "        techniques_res[name].append(count_word_occurrences(local, t))\n",
    "\n",
    "technique_df = pd.DataFrame.from_dict(techniques_res)\n",
    "metric_df = pd.DataFrame.from_dict(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       metrics  free-for-dev  netdata  act  gitea  kong  sentry  cli  dokku  \\\n",
      "0  acquisition             0        0    0      0     0       0    0      0   \n",
      "1   activation             0        0    0      0     0       0    0      0   \n",
      "2    retention             0        0    0      0     0       0    0      0   \n",
      "3      referal             0        0    0      0     0       0    0      0   \n",
      "4      revenue             0        0    0      0     0       0    0      0   \n",
      "\n",
      "   90DaysOfDevOps  ...  watchtower  wtf  argo-cd  sops  kubesphere  apisix  \\\n",
      "0               0  ...           0    0        0     0           0       0   \n",
      "1               0  ...           0    0        0     0           0       0   \n",
      "2               0  ...           0    0        0     0           0       0   \n",
      "3               0  ...           0    0        0     0           0       0   \n",
      "4               0  ...           0    0        0     0           0       0   \n",
      "\n",
      "   onedev  walle-web  lynis  kubeshark  \n",
      "0       0          0      0          0  \n",
      "1       0          0      0          0  \n",
      "2       0          0      0          0  \n",
      "3       0          0      0          0  \n",
      "4       0          0      0          0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(technique_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
